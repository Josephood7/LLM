{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XMJ8aiLHVKGk","outputId":"c2c79177-f160-494f-8168-f314ff1163f6","trusted":true},"outputs":[],"source":["import json\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","from tqdm import tqdm\n","from transformers import BertTokenizerFast, AutoTokenizer, BertModel, AutoModel\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","def tokenize_data(file_path):\n","  def read_data(file_path):\n","    with open(file_path, 'r', encoding=\"utf-8\") as f:\n","        data_in = json.load(f)\n","    datas = [{'id': x[0],'text1': x[1], 'text2': x[2], 'relation': x[3]} for x in data_in]\n","    return datas\n","\n","  datas = read_data(file_path)\n","  #tokenizer = BertTokenizerFast.from_pretrained(\"ckiplab/albert-base-chinese\")\n","  #tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n","  #tokenizer = AutoTokenizer.from_pretrained(\"luhua/chinese_pretrain_mrc_roberta_wwm_ext_large\")\n","  tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-electra-180g-large-discriminator\")\n","  text1_tokenized = tokenizer([data[\"text1\"] for data in datas], add_special_tokens=False, truncation = True)\n","  text2_tokenized = tokenizer([data[\"text2\"] for data in datas], add_special_tokens=False,truncation = True)\n","\n","  return datas, text1_tokenized, text2_tokenized\n","\n","train_datas, train_text1_tokenized, train_text2_tokenized = tokenize_data('/kaggle/input/nlp-final/Final Project Task 1/team_train.json')\n","dev_datas, dev_text1_tokenized, dev_text2_tokenized = tokenize_data('/kaggle/input/nlp-final/Final Project Task 1/team_dev.json')\n","print(len(train_datas))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8sTknMz37oaY","trusted":true},"outputs":[],"source":["class text_Dataset(Dataset):\n","    def __init__(self, datas, tokenized_text1, tokenized_text2):\n","        self.datas = datas\n","        self.tokenized_text1 = tokenized_text1\n","        self.tokenized_text2 = tokenized_text2\n","        self.max_len = 254\n","        # Input sequence length = [CLS] + text1 + [SEP] + text2 + [SEP]\n","        self.max_seq_len = 1 + self.max_len + 1 + self.max_len + 1\n","\n","    def __len__(self):\n","        return len(self.datas)\n","\n","    def __getitem__(self, idx):\n","        data = self.datas[idx]\n","        id = data['id']\n","        label = data['relation']\n","        tokenized_text1 = self.tokenized_text1[idx]\n","        tokenized_text2 = self.tokenized_text2[idx]\n","\n","        # Add special tokens (101: CLS, 102: SEP)\n","        input_ids_text1 = [101] + tokenized_text1.ids[:self.max_len] + [102]\n","        input_ids_text2 = tokenized_text2.ids[:self.max_len] + [102]\n","\n","        # Pad sequence and obtain inputs to model\n","        input_ids, token_type_ids, attention_mask = self.padding(input_ids_text1, input_ids_text2)\n","        return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), label\n","\n","    def padding(self, input_ids_text1, input_ids_text2):\n","        # Pad zeros if sequence length is shorter than max_seq_len\n","        padding_len = self.max_seq_len - len(input_ids_text1) - len(input_ids_text2)\n","        # Indices of input sequence tokens in the vocabulary\n","        input_ids = input_ids_text1 + input_ids_text2 + [0] * padding_len\n","        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n","        token_type_ids = [0] * len(input_ids_text1) + [1] * len(input_ids_text2) + [0] * padding_len\n","        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n","        attention_mask = [1] * (len(input_ids_text1) + len(input_ids_text2)) + [0] * padding_len\n","\n","        return input_ids, token_type_ids, attention_mask\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9SnVVr1zEAJj","trusted":true},"outputs":[],"source":["class BERTClassifier(nn.Module):\n","    def __init__(self, num_classes):\n","        super(BERTClassifier, self).__init__()\n","        #self.bert = BertModel.from_pretrained(\"ckiplab/albert-base-chinese\")\n","        #self.bert = BertModel.from_pretrained(\"bert-base-chinese\")\n","        #self.bert = BertModel.from_pretrained(\"luhua/chinese_pretrain_mrc_roberta_wwm_ext_large\")\n","        self.bert = AutoModel.from_pretrained(\"hfl/chinese-electra-180g-large-discriminator\")\n","        self.d_dim = self.bert.config.hidden_size\n","        self.dropout = nn.Dropout(0.25)\n","        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n","\n","    def forward(self, input_ids, token_type_ids, attention_mask):\n","        output = self.bert(input_ids=input_ids, token_type_ids = token_type_ids, attention_mask=attention_mask)\n","        #For ALBERT, BERT, RoBERTa\n","            #x = output.pooler_output\n","        #For ELECTRA\n","        x = output.last_hidden_state[:,0,:]#cls last_hidden\n","        x = self.dropout(x)\n","        x = self.fc(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jCX--BnltVf2","trusted":true},"outputs":[],"source":["from accelerate import Accelerator\n","\n","class Trainer():\n","    def __init__(self, config, train_loader, valid_loader):\n","        _, self.n_epochs, self.lr, self.patience, self.model = config.values()\n","        self.train_loader = train_loader\n","        self.valid_loader = valid_loader\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=1e-5)\n","        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, T_0=10, T_mult=2)\n","        self.gradient_accumulation_steps = 16\n","        self.accelerator = Accelerator(gradient_accumulation_steps = self.gradient_accumulation_steps)\n","        self.model, self.optimizer, self.train_loader = self.accelerator.prepare(self.model, self.optimizer, self.train_loader) \n","\n","    def train(self):\n","        stale = 0\n","        best_acc = 0\n","        _exp_name = \"ver\"\n","\n","        for epoch in range(self.n_epochs):\n","            print(f'Epoch_{epoch} starts!!')\n","            self.model.train()\n","            train_loss = []\n","            train_accs = []\n","            for batch in self.train_loader:\n","                data = [i.to(device) for i in batch]\n","                input_ids, token_type_ids, attention_mask, labels = data\n","                out = self.model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n","\n","                loss = self.criterion(out, labels.to(device))\n","                acc = (out.argmax(dim=-1) == labels.to(device)).float().mean()\n","\n","                self.optimizer.zero_grad()\n","                loss.backward()\n","                self.optimizer.step()\n","\n","                train_loss.append(loss.item())\n","                train_accs.append(acc)\n","\n","            train_loss = sum(train_loss) / len(train_loss)\n","            train_acc = sum(train_accs) / len(train_accs)\n","            print(f\"[ Train | {epoch + 1:03d}/{self.n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n","\n","            # validation\n","            self.model.eval()\n","            valid_loss = []\n","            valid_accs = []\n","\n","            for batch in self.valid_loader:\n","                data = [i.to(device) for i in batch]\n","                input_ids, token_type_ids, attention_mask, labels = data\n","                \n","                with torch.no_grad():\n","                    out = self.model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n","\n","                loss = self.criterion(out, labels.to(device))\n","                acc = (out.argmax(dim=-1) == labels.to(device)).float().mean()\n","\n","                valid_loss.append(loss.item())\n","                valid_accs.append(acc)\n","            valid_loss = sum(valid_loss) / len(valid_loss)\n","            valid_acc = sum(valid_accs) / len(valid_accs)\n","\n","            print(f\"[ Valid | {epoch + 1:03d}/{self.n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n","\n","            if valid_acc > best_acc:\n","                with open(f\"./{_exp_name}_log.txt\",\"a\"):\n","                    print(f\"[ Valid | {epoch + 1:03d}/{self.n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n","            else:\n","                with open(f\"./{_exp_name}_log.txt\",\"a\"):\n","                    print(f\"[ Valid | {epoch + 1:03d}/{self.n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n","\n","            if valid_acc > best_acc:\n","                print(f\"Best model found at epoch {epoch}, saving model\")\n","                torch.save(self.model.state_dict(), f\"{_exp_name}_best.ckpt\") # only save best to prevent output memory exceed error\n","                best_acc = valid_acc\n","                stale = 0\n","            else:\n","                stale += 1\n","                if stale > self.patience:\n","                    print(f\"No improvment {self.patience} consecutive epochs, early stopping\")\n","                    break\n","\n","            self.scheduler.step()\n","        #save checkpoint\n","            torch.save({\n","                    'epoch': epoch,\n","                    'model_state_dict': self.model.state_dict(),\n","                    'optimizer_state_dict': self.optimizer.state_dict(),\n","                    'scheduler_state_dict': self.scheduler.state_dict(),\n","                    'train_loss': train_loss,\n","                    'valid_loss': valid_loss,\n","                    'best_acc': best_acc,\n","                    }, f\"model.pt\")\n","        return\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ufqJFcbbteLc","outputId":"6ef1e898-189b-4f01-cf61-08b8f219ac15","trusted":true},"outputs":[],"source":["config = {'batch_size': 4, #8\n","          'n_epochs': 100,\n","          'lr': 1e-5,\n","          'patience': 40,\n","          'model': BERTClassifier(num_classes = 3).to(device)\n","          }\n","\n","train_set = text_Dataset(train_datas, train_text1_tokenized, train_text2_tokenized)\n","train_loader = DataLoader(train_set, batch_size=config['batch_size'], shuffle=True, num_workers=0, pin_memory=True)\n","dev_set = text_Dataset(dev_datas, dev_text1_tokenized, dev_text2_tokenized)\n","dev_loader = DataLoader(dev_set, batch_size=config['batch_size'], shuffle=True, num_workers=0, pin_memory=True)\n","\n","print(len(train_loader), len(dev_loader))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dXt02kyuC-u","outputId":"66069334-9445-4e14-80b2-2b927c5138f2","trusted":true},"outputs":[],"source":["trainer = Trainer(config, train_loader, dev_loader)\n","trainer.train()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4993335,"sourceId":8393807,"sourceType":"datasetVersion"},{"datasetId":5036704,"sourceId":8451511,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
