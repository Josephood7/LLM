{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T10:02:13.494846Z","iopub.status.busy":"2024-05-19T10:02:13.494273Z","iopub.status.idle":"2024-05-19T10:02:23.263120Z","shell.execute_reply":"2024-05-19T10:02:23.261950Z","shell.execute_reply.started":"2024-05-19T10:02:13.494818Z"},"id":"XMJ8aiLHVKGk","outputId":"c2c79177-f160-494f-8168-f314ff1163f6","papermill":{"duration":10.794975,"end_time":"2024-05-14T18:34:48.439905","exception":false,"start_time":"2024-05-14T18:34:37.644930","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["import json\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","from tqdm import tqdm\n","from transformers import BertTokenizerFast, AutoTokenizer, BertModel, AutoModel\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","def tokenize_data(file_path, mode):\n","  def read_data(file_path):\n","    with open(file_path, 'r', encoding=\"utf-8\") as f:\n","        data_in = json.load(f)\n","    if mode == 'test':\n","        datas = [{'id': x[0],'text1': x[1], 'text2': x[2]} for x in data_in]\n","    else:\n","        datas = [{'id': x[0],'text1': x[1], 'text2': x[2], 'relation': x[3]} for x in data_in]\n","    return datas\n","\n","  datas = read_data(file_path)\n","  #tokenizer = BertTokenizerFast.from_pretrained(\"ckiplab/albert-base-chinese\")\n","  #tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n","  #tokenizer = AutoTokenizer.from_pretrained(\"luhua/chinese_pretrain_mrc_roberta_wwm_ext_large\")\n","  tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-electra-180g-large-discriminator\")\n","  \n","  text1_tokenized = tokenizer([data[\"text1\"] for data in datas], add_special_tokens=False, truncation=True)\n","  text2_tokenized = tokenizer([data[\"text2\"] for data in datas], add_special_tokens=False, truncation=True)\n","\n","  return datas, text1_tokenized, text2_tokenized\n","\n","\n","test_datas, test_text1_tokenized, test_text2_tokenized = tokenize_data('/kaggle/input/nlp-final/Final Project Task 1/team_test.json', mode = 'test')\n","print(len(test_datas))"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T10:02:23.265832Z","iopub.status.busy":"2024-05-19T10:02:23.264867Z","iopub.status.idle":"2024-05-19T10:02:23.281959Z","shell.execute_reply":"2024-05-19T10:02:23.278146Z","shell.execute_reply.started":"2024-05-19T10:02:23.265805Z"},"id":"8sTknMz37oaY","papermill":{"duration":0.018185,"end_time":"2024-05-14T18:34:48.462091","exception":false,"start_time":"2024-05-14T18:34:48.443906","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class text_Dataset(Dataset):\n","    def __init__(self, datas, tokenized_text1, tokenized_text2,mode):\n","        self.datas = datas\n","        self.tokenized_text1 = tokenized_text1\n","        self.tokenized_text2 = tokenized_text2\n","        self.max_len = 254\n","        self.mode = mode\n","\n","        # Input sequence length = [CLS] + text1 + [SEP] + text2 + [SEP]\n","        self.max_seq_len = 1 + self.max_len + 1 + self.max_len + 1\n","\n","    def __len__(self):\n","        return len(self.datas)\n","\n","    def __getitem__(self, idx):\n","        data = self.datas[idx]\n","        ID = data['id']\n","        tokenized_text1 = self.tokenized_text1[idx]\n","        tokenized_text2 = self.tokenized_text2[idx]\n","\n","        # add special tokens (101: CLS, 102: SEP)\n","        input_ids_text1 = [101] + tokenized_text1.ids[:self.max_len] + [102]\n","        input_ids_text2 = tokenized_text2.ids[:self.max_len] + [102]\n","\n","        # Pad sequence and obtain inputs to model\n","        input_ids, token_type_ids, attention_mask = self.padding(input_ids_text1, input_ids_text2)\n","        if self.mode != 'test':\n","            label = data['relation']\n","            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), label\n","        else:\n","            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), ID\n","\n","    def padding(self, input_ids_text1, input_ids_text2):\n","        # Pad zeros if sequence length is shorter than max_seq_len\n","        padding_len = self.max_seq_len - len(input_ids_text1) - len(input_ids_text2)\n","        # Indices of input sequence tokens in the vocabulary\n","        input_ids = input_ids_text1 + input_ids_text2 + [0] * padding_len\n","        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n","        token_type_ids = [0] * len(input_ids_text1) + [1] * len(input_ids_text2) + [0] * padding_len\n","        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n","        attention_mask = [1] * (len(input_ids_text1) + len(input_ids_text2)) + [0] * padding_len\n","\n","        return input_ids, token_type_ids, attention_mask\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T10:02:23.284463Z","iopub.status.busy":"2024-05-19T10:02:23.283771Z","iopub.status.idle":"2024-05-19T10:02:23.294510Z","shell.execute_reply":"2024-05-19T10:02:23.293597Z","shell.execute_reply.started":"2024-05-19T10:02:23.284431Z"},"id":"9SnVVr1zEAJj","papermill":{"duration":0.015626,"end_time":"2024-05-14T18:34:48.481266","exception":false,"start_time":"2024-05-14T18:34:48.465640","status":"completed"},"tags":[],"trusted":true},"outputs":[],"source":["class BERTClassifier(nn.Module):\n","    def __init__(self, num_classes):\n","        super(BERTClassifier, self).__init__()\n","        #self.bert = BertModel.from_pretrained(\"ckiplab/albert-base-chinese\")\n","        #self.bert = BertModel.from_pretrained(\"bert-base-chinese\")\n","        #self.bert = BertModel.from_pretrained(\"luhua/chinese_pretrain_mrc_roberta_wwm_ext_large\")\n","        self.bert = AutoModel.from_pretrained(\"hfl/chinese-electra-180g-large-discriminator\")\n","        self.d_dim = self.bert.config.hidden_size\n","        self.dropout = nn.Dropout(0.25)\n","        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n","\n","    def forward(self, input_ids, token_type_ids, attention_mask):\n","        output = self.bert(input_ids=input_ids, token_type_ids = token_type_ids, attention_mask=attention_mask)\n","        #For ALBERT, BERT, RoBERTa\n","            #x = output.pooler_output\n","        #For ELECTRA\n","        x = output.last_hidden_state[:,0,:]#cls last_hidden\n","        x = self.dropout(x)\n","        x = self.fc(x)\n","        return x"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T10:02:23.296972Z","iopub.status.busy":"2024-05-19T10:02:23.296704Z","iopub.status.idle":"2024-05-19T10:02:23.306929Z","shell.execute_reply":"2024-05-19T10:02:23.306150Z","shell.execute_reply.started":"2024-05-19T10:02:23.296943Z"},"id":"ufqJFcbbteLc","outputId":"6ef1e898-189b-4f01-cf61-08b8f219ac15","papermill":{"duration":9.157262,"end_time":"2024-05-14T18:34:57.675542","exception":false,"start_time":"2024-05-14T18:34:48.518280","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["815\n"]}],"source":["test_set = text_Dataset(test_datas, test_text1_tokenized, test_text2_tokenized, mode = 'test')\n","test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0, pin_memory=True)\n","print(len(test_loader))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T10:04:31.913026Z","iopub.status.busy":"2024-05-19T10:04:31.912698Z","iopub.status.idle":"2024-05-19T10:05:27.848602Z","shell.execute_reply":"2024-05-19T10:05:27.847716Z","shell.execute_reply.started":"2024-05-19T10:04:31.913002Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 815/815 [00:53<00:00, 15.15it/s]\n"]}],"source":["import numpy as np\n","model_best = BERTClassifier(num_classes = 3).to(device)\n","model_best.load_state_dict(torch.load(\"ver_best.ckpt\"))\n","model_best.eval()\n","prediction = []\n","with torch.no_grad():\n","    for batch in tqdm(test_loader):\n","        data = [i.to(device) for i in batch]\n","        #print(data)\n","        input_ids, token_type_ids, attention_mask, ID = data\n","        test_pred = model_best(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n","        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n","        prediction += test_label.tolist()\n","        "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T10:05:27.850474Z","iopub.status.busy":"2024-05-19T10:05:27.850191Z","iopub.status.idle":"2024-05-19T10:05:27.855272Z","shell.execute_reply":"2024-05-19T10:05:27.854417Z","shell.execute_reply.started":"2024-05-19T10:05:27.850449Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 2, 1, 0, 1, 2, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 0, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 0, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 0, 1, 1, 0, 2, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 0, 2, 1, 1, 1, 2, 1, 2, 0, 1, 1, 1, 2, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 0, 2, 2, 2, 0, 1, 2, 0, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 0, 0, 1, 1, 1, 0, 0, 2, 1, 1, 0, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 0, 2, 2, 2, 2, 0, 1, 1, 0, 1, 0, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 2, 1, 1, 1, 0, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 0, 1, 0, 2, 0, 1, 0, 1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2, 0, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 2, 1, 1, 0, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 0, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 0, 0, 2, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 2, 1, 1, 0, 1, 1, 0, 2, 1, 1, 1, 1, 2, 2, 1, 0, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 0, 2, 1, 1, 0, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 0, 2, 1, 2, 1, 1, 2, 0, 2, 1, 0, 1, 1, 2, 1, 1, 1, 1, 2, 0, 1, 1, 2, 2, 1, 2, 2, 2, 1, 0, 1, 2, 1, 1, 2, 1, 0, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 0, 2, 2, 1, 1, 1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 1, 0, 1, 1, 2, 1, 0, 1, 1, 0, 1, 2, 1, 1, 2, 1, 0, 1, 1, 0, 2, 2, 2, 2, 2, 1, 2, 1, 0, 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 0, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 0, 1, 2, 0, 2, 1, 1, 1, 1, 2, 0, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 1, 0, 2, 0, 1, 2, 2, 2, 0, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 0, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 0, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 0, 2, 1, 0, 1, 2, 1, 2, 1, 1, 0, 1, 0, 2, 2, 2, 0, 1, 1, 2, 2, 1, 0, 1, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1]\n"]}],"source":["print(prediction)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T10:04:19.520519Z","iopub.status.busy":"2024-05-19T10:04:19.519750Z","iopub.status.idle":"2024-05-19T10:04:19.851295Z","shell.execute_reply":"2024-05-19T10:04:19.850448Z","shell.execute_reply.started":"2024-05-19T10:04:19.520486Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","#create test csv\n","df = pd.DataFrame()\n","df[\"ID\"] = [i+1 for i in range(len(test_set))]\n","df[\"y_pred\"] = prediction\n","df.to_csv(\"ELECTRA_v1.csv\",index = False)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4993335,"sourceId":8393807,"sourceType":"datasetVersion"},{"datasetId":5036704,"sourceId":8451511,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":null,"end_time":null,"environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-14T18:34:34.606435","version":"2.5.0"}},"nbformat":4,"nbformat_minor":5}
